{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font size=\"5\">Survey on Machine Learning Algorithms for IMDB review Sentiment Classification</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "#### Source Data Access\n",
    "    - Acquiring IMDB movie reviews with sentiment polarity labels dataset from Kaggle in csv format\n",
    "#### Data Preprocessing\n",
    "    - Remove HTML Tags using HTML parser\n",
    "    - Remove contents inside square brackets using regex\n",
    "    - Remove special characters using regex\n",
    "    - Remove stopwords using nltk\n",
    "    - Stemming of words using nltk\n",
    "#### Sampling of dataset\n",
    "    - 5k positive reviews and 5k negative reviews (total 10k reviews) for training. Remaining 40k reviews for testing.\n",
    "    - 10k positive reviews and 10k negative reviews (total 20k reviews) for training. Remaining 30k reviews for testing.\n",
    "    - 15k positive reviews and 15k negative reviews (total 30k reviews) for training. Remaining 20k reviews for testing.\n",
    "#### Word Vectorisation\n",
    "    - Count vectoriser - Bag of Words model\n",
    "    - Tfidf Vectoriser - Term Frequency - Inverse Document Frequency model\n",
    "#### Label Binarizer\n",
    "    - Converting the sentiment labels to 1s and 0s (Positive and Negative)\n",
    "#### Modelling and testing \n",
    "    - Logistic Regression\n",
    "    - Multinomial Naives Bayes\n",
    "    - Support Vector Machines (poly)\n",
    "    - Support Vector Machines (linear)\n",
    "    - Support Vector Machines (rbf)\n",
    "#### Final Report \n",
    "    - Accuracy Table\n",
    "    - Classification report using sklearn.metrics\n",
    "#### Inference\n",
    "    - Algorithms with maximum accuracy\n",
    "    - Precision score for those algorithms\n",
    "    - A word on runtime and cost,time effective algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(str(os.getcwd()),'Desktop','IMDB Dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review = df.review.apply(strip_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     One of the other reviewers has mentioned that ...\n",
       "1     A wonderful little production. The filming tec...\n",
       "2     I thought this was a wonderful way to spend ti...\n",
       "3     Basically there's a family where a little boy ...\n",
       "4     Petter Mattei's \"Love in the Time of Money\" is...\n",
       "5     Probably my all-time favorite movie, a story o...\n",
       "6     I sure would like to see a resurrection of a u...\n",
       "7     This show was an amazing, fresh & innovative i...\n",
       "8     Encouraged by the positive comments about this...\n",
       "9     If you like original gut wrenching laughter yo...\n",
       "10    Phil the Alien is one of those quirky films wh...\n",
       "11    I saw this movie when I was about 12 when it c...\n",
       "12    So im not a big fan of Boll's work but then ag...\n",
       "13    The cast played Shakespeare.Shakespeare lost.I...\n",
       "14    This a fantastic movie of three prisoners who ...\n",
       "15    Kind of drawn in by the erotic scenes, only to...\n",
       "16    Some films just simply should not be remade. T...\n",
       "17    This movie made it into one of my top 10 most ...\n",
       "18    I remember this film,it was the first film i h...\n",
       "19    An awful film! It must have been up against so...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove contents inside a square bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_square_brackets(text):\n",
    "    pattern = re.compile(r'\\[[^]]*\\]')\n",
    "    matches = pattern.finditer(text)\n",
    "    for match in matches:\n",
    "        print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(3108, 3131), match='[SPOILER . . . I guess]'>\n",
      "<_sre.SRE_Match object; span=(3796, 4076), match=\"[DVD tip: as with the simultaneously released Vis>\n",
      "<_sre.SRE_Match object; span=(179, 185), match='[100%]'>\n",
      "<_sre.SRE_Match object; span=(427, 432), match='[70%]'>\n",
      "<_sre.SRE_Match object; span=(535, 540), match='[90%]'>\n",
      "<_sre.SRE_Match object; span=(677, 683), match='[100%]'>\n",
      "<_sre.SRE_Match object; span=(756, 761), match='[91%]'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='[WARNING: CONTAINS SPOILERS]'>\n",
      "<_sre.SRE_Match object; span=(0, 93), match=\"[I saw this movie once late on a public tv statio>\n",
      "<_sre.SRE_Match object; span=(492, 503), match='[soon Mimi]'>\n",
      "<_sre.SRE_Match object; span=(16, 22), match='[1986]'>\n",
      "<_sre.SRE_Match object; span=(71, 86), match='[James Belushi]'>\n",
      "<_sre.SRE_Match object; span=(188, 205), match='[Christine Tucci]'>\n",
      "<_sre.SRE_Match object; span=(46, 86), match='[warning: may be considered a \"spoiler\"]'>\n",
      "<_sre.SRE_Match object; span=(671, 675), match='[es]'>\n",
      "<_sre.SRE_Match object; span=(2784, 2874), match='[actress Annabelle Weenick, who also served as th>\n",
      "<_sre.SRE_Match object; span=(3370, 3422), match='[and his nonstop consumption of chocolate popsicl>\n",
      "<_sre.SRE_Match object; span=(5212, 5237), match='[and implied Vietnam vet]'>\n",
      "<_sre.SRE_Match object; span=(8212, 8423), match=\"[who seems to be inspired by the Donald Sutherlan>\n",
      "<_sre.SRE_Match object; span=(508, 538), match='[actually it always had these]'>\n",
      "<_sre.SRE_Match object; span=(355, 360), match='[sic]'>\n",
      "<_sre.SRE_Match object; span=(40, 89), match='[or hospice, if I want to be politically correct]>\n",
      "<_sre.SRE_Match object; span=(90, 137), match='[which ass coined this asinine phrase, anyway?]'>\n",
      "<_sre.SRE_Match object; span=(429, 469), match=\"[which probably wasn't so golden anyway]\">\n",
      "<_sre.SRE_Match object; span=(166, 179), match='[Tipper Gore]'>\n",
      "<_sre.SRE_Match object; span=(1242, 1248), match='[1932]'>\n",
      "<_sre.SRE_Match object; span=(496, 545), match='[\"do you think he\\'s compensating for something?\">\n",
      "<_sre.SRE_Match object; span=(189, 224), match=\"[warning, I'm summarizing the plot]\">\n",
      "<_sre.SRE_Match object; span=(322, 400), match='[ avoiding spoiler - everyone has to see Oliver! >\n",
      "<_sre.SRE_Match object; span=(0, 626), match='[ as a new resolution for this year 2005, i decid>\n",
      "<_sre.SRE_Match object; span=(432, 449), match='[who directed it]'>\n",
      "<_sre.SRE_Match object; span=(505, 524), match='[Elton John to you]'>\n",
      "<_sre.SRE_Match object; span=(559, 590), match='[and never officially released]'>\n",
      "<_sre.SRE_Match object; span=(733, 795), match=\"[said to have 'created glam rock' with her use of>\n",
      "<_sre.SRE_Match object; span=(1088, 1101), match='[out of five]'>\n",
      "<_sre.SRE_Match object; span=(49, 57), match='[Xizhao]'>\n",
      "<_sre.SRE_Match object; span=(1197, 1203), match='[1976]'>\n",
      "<_sre.SRE_Match object; span=(765, 788), match='[or possibly Gourmands]'>\n",
      "<_sre.SRE_Match object; span=(1232, 1306), match='[Helga (about a dagger): \"Will be used by another>\n",
      "<_sre.SRE_Match object; span=(780, 800), match='[*cough SONY cough*]'>\n",
      "<_sre.SRE_Match object; span=(299, 311), match='[presumably]'>\n",
      "<_sre.SRE_Match object; span=(213, 216), match='[1]'>\n",
      "<_sre.SRE_Match object; span=(812, 818), match='[*Rec]'>\n",
      "<_sre.SRE_Match object; span=(103, 111), match='[proves]'>\n",
      "<_sre.SRE_Match object; span=(468, 479), match='[the smell]'>\n",
      "<_sre.SRE_Match object; span=(681, 707), match='[cut to black; next story]'>\n",
      "<_sre.SRE_Match object; span=(3401, 3409), match='[Berman]'>\n",
      "<_sre.SRE_Match object; span=(3422, 3429), match='[Braga]'>\n",
      "<_sre.SRE_Match object; span=(626, 646), match='[Not worth a rating]'>\n",
      "<_sre.SRE_Match object; span=(217, 272), match='[she must have refused to or had ordered major ch>\n",
      "<_sre.SRE_Match object; span=(847, 864), match='[Hamton & Plucky]'>\n",
      "<_sre.SRE_Match object; span=(908, 923), match='[Fifi & Johnny]'>\n",
      "<_sre.SRE_Match object; span=(1285, 1306), match='[i.e. Eyes Wide Shut]'>\n",
      "<_sre.SRE_Match object; span=(515, 518), match='[R]'>\n",
      "<_sre.SRE_Match object; span=(2560, 2580), match='[big dramatic pause]'>\n",
      "<_sre.SRE_Match object; span=(220, 239), match='[I can kill ANY TV]'>\n",
      "<_sre.SRE_Match object; span=(236, 242), match='[1965]'>\n",
      "<_sre.SRE_Match object; span=(269, 272), match='[1]'>\n",
      "<_sre.SRE_Match object; span=(358, 361), match='[2]'>\n",
      "<_sre.SRE_Match object; span=(477, 480), match='[3]'>\n",
      "<_sre.SRE_Match object; span=(801, 804), match='[4]'>\n",
      "<_sre.SRE_Match object; span=(186, 189), match='[?]'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='[CONTAINS SPOILERS!!!]'>\n",
      "<_sre.SRE_Match object; span=(755, 1157), match=\"[And while I don't mind movies that are cheerfull>\n",
      "<_sre.SRE_Match object; span=(1733, 1747), match='[surprisingly]'>\n",
      "<_sre.SRE_Match object; span=(395, 401), match='[1933]'>\n",
      "<_sre.SRE_Match object; span=(653, 659), match='[1965]'>\n",
      "<_sre.SRE_Match object; span=(2738, 2744), match='[1933]'>\n",
      "<_sre.SRE_Match object; span=(424, 429), match='[act]'>\n",
      "<_sre.SRE_Match object; span=(1431, 1437), match='[1968]'>\n",
      "<_sre.SRE_Match object; span=(863, 869), match='[1939]'>\n",
      "<_sre.SRE_Match object; span=(934, 940), match='[1936]'>\n",
      "<_sre.SRE_Match object; span=(19, 64), match='[remake of the excellent \"Day of the Jackal\"]'>\n",
      "<_sre.SRE_Match object; span=(86, 109), match='[and a boatload budget]'>\n",
      "<_sre.SRE_Match object; span=(529, 535), match='[of 4]'>\n",
      "<_sre.SRE_Match object; span=(1111, 1126), match='[black-market-]'>\n",
      "<_sre.SRE_Match object; span=(1659, 1673), match='[ \"blackfeet\"]'>\n",
      "<_sre.SRE_Match object; span=(168, 174), match='[1936]'>\n",
      "<_sre.SRE_Match object; span=(395, 401), match='[1946]'>\n",
      "<_sre.SRE_Match object; span=(1129, 1135), match='[1946]'>\n",
      "<_sre.SRE_Match object; span=(1119, 1136), match='[Jayne Mansfield]'>\n",
      "<_sre.SRE_Match object; span=(1465, 1515), match=\"[though not all that easy to tell what's going on>\n",
      "<_sre.SRE_Match object; span=(901, 959), match='[ie. the Hapsburgs, WW-I, WW-II, Fascism, Communi>\n",
      "<_sre.SRE_Match object; span=(1082, 1124), match=\"[the 'secret' diary and family philosophy]\">\n",
      "<_sre.SRE_Match object; span=(433, 504), match=\"[and this comes from a guy that can't stand the s>\n",
      "<_sre.SRE_Match object; span=(532, 635), match='[\"And then you put the wafer into their mouths. A>\n",
      "<_sre.SRE_Match object; span=(442, 504), match='[The actress looks as if she has probably done so>\n",
      "<_sre.SRE_Match object; span=(747, 779), match='[a magnificently handsome woman]'>\n",
      "<_sre.SRE_Match object; span=(1315, 1330), match='[such as it is]'>\n",
      "<_sre.SRE_Match object; span=(171, 233), match='[Possible Spoilers For Those Who Are Unfamiliar W>\n",
      "<_sre.SRE_Match object; span=(936, 947), match='[/Spoilers]'>\n",
      "<_sre.SRE_Match object; span=(1166, 1172), match='[1954]'>\n",
      "<_sre.SRE_Match object; span=(174, 180), match='[1976]'>\n",
      "<_sre.SRE_Match object; span=(199, 205), match='[1977]'>\n",
      "<_sre.SRE_Match object; span=(586, 594), match='[boring]'>\n",
      "<_sre.SRE_Match object; span=(346, 394), match='[notice the boat hitting the reef in particular]'>\n",
      "<_sre.SRE_Match object; span=(1914, 1920), match='[1970]'>\n",
      "<_sre.SRE_Match object; span=(858, 861), match='[a]'>\n",
      "<_sre.SRE_Match object; span=(952, 955), match='[b]'>\n",
      "<_sre.SRE_Match object; span=(1106, 1109), match='[b]'>\n",
      "<_sre.SRE_Match object; span=(260, 298), match='[filled of course with silver bullets]'>\n",
      "<_sre.SRE_Match object; span=(162, 293), match='[overcoming fear of The Stranger, learning to ris>\n",
      "<_sre.SRE_Match object; span=(372, 408), match='[who thinks most A/A is violent pap]'>\n",
      "<_sre.SRE_Match object; span=(1077, 1120), match='[or rents it or sees it in a cable listing]'>\n",
      "<_sre.SRE_Match object; span=(100, 120), match='[INSERT ANIMAL HERE]'>\n",
      "<_sre.SRE_Match object; span=(274, 320), match='[what the world could have been like backthen]'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='[Warning contains spoilers]'>\n",
      "<_sre.SRE_Match object; span=(1101, 1126), match='[potential spoiler alert]'>\n",
      "<_sre.SRE_Match object; span=(1072, 1168), match=\"[The closing titles also have some of the most on>\n",
      "<_sre.SRE_Match object; span=(1448, 1454), match='[1955]'>\n",
      "<_sre.SRE_Match object; span=(1488, 1494), match='[1955]'>\n",
      "<_sre.SRE_Match object; span=(1815, 1821), match='[1946]'>\n",
      "<_sre.SRE_Match object; span=(1924, 1930), match='[1982]'>\n",
      "<_sre.SRE_Match object; span=(2011, 2017), match='[1961]'>\n",
      "<_sre.SRE_Match object; span=(1544, 1550), match='[1/10]'>\n",
      "<_sre.SRE_Match object; span=(53, 63), match='[original]'>\n",
      "<_sre.SRE_Match object; span=(692, 699), match='[their]'>\n",
      "<_sre.SRE_Match object; span=(677, 683), match='[1961]'>\n",
      "<_sre.SRE_Match object; span=(702, 708), match='[1975]'>\n",
      "<_sre.SRE_Match object; span=(1480, 1486), match='[1972]'>\n",
      "<_sre.SRE_Match object; span=(1989, 1995), match='[1971]'>\n",
      "<_sre.SRE_Match object; span=(2470, 2476), match='[1997]'>\n",
      "<_sre.SRE_Match object; span=(1315, 1321), match='[1966]'>\n",
      "<_sre.SRE_Match object; span=(1643, 1649), match='[1968]'>\n",
      "<_sre.SRE_Match object; span=(226, 240), match='[of the movie]'>\n",
      "<_sre.SRE_Match object; span=(605, 629), match='[ATC(LA/AC) USN Retired]'>\n",
      "<_sre.SRE_Match object; span=(1026, 1066), match='[Which means lots of work nevertheless!]'>\n",
      "<_sre.SRE_Match object; span=(0, 19), match='[possible spoilers]'>\n",
      "<_sre.SRE_Match object; span=(46, 53), match='[#4/10]'>\n",
      "<_sre.SRE_Match object; span=(2569, 2575), match='[1975]'>\n",
      "<_sre.SRE_Match object; span=(3233, 3239), match='[2008]'>\n",
      "<_sre.SRE_Match object; span=(4257, 4267), match='[Stallone]'>\n",
      "<_sre.SRE_Match object; span=(129, 185), match='[From Here To Eternity, Picnic, The World of Suzi>\n",
      "<_sre.SRE_Match object; span=(227, 266), match='[including the famous Ambassador Hotel]'>\n",
      "<_sre.SRE_Match object; span=(324, 371), match='[Kojak, Streets of San Francisco, The Avengers]'>\n",
      "<_sre.SRE_Match object; span=(369, 413), match=\"[not that anyone's going to believe that...]\">\n",
      "<_sre.SRE_Match object; span=(1803, 1851), match='[who look like rejects from \"The Mummy Returns\"]'>\n",
      "<_sre.SRE_Match object; span=(323, 332), match='[McGowan]'>\n",
      "<_sre.SRE_Match object; span=(407, 487), match=\"[Scarlett in both novels had green eyes and even >\n",
      "<_sre.SRE_Match object; span=(3258, 3293), match='[Resend. Revised. ruby_fff 2/22/01]'>\n",
      "<_sre.SRE_Match object; span=(423, 523), match='[I follow Japanese custom of presenting the famil>\n",
      "<_sre.SRE_Match object; span=(620, 857), match='[Also, I shall not use the movie character names >\n",
      "<_sre.SRE_Match object; span=(145, 151), match='[1969]'>\n",
      "<_sre.SRE_Match object; span=(164, 170), match='[1977]'>\n",
      "<_sre.SRE_Match object; span=(1603, 1609), match='[1974]'>\n",
      "<_sre.SRE_Match object; span=(320, 326), match='[1980]'>\n",
      "<_sre.SRE_Match object; span=(119, 129), match='[Duchovny]'>\n",
      "<_sre.SRE_Match object; span=(278, 284), match='[Pitt]'>\n",
      "<_sre.SRE_Match object; span=(2683, 2688), match='[ing]'>\n",
      "<_sre.SRE_Match object; span=(2697, 2702), match='[ing]'>\n",
      "<_sre.SRE_Match object; span=(98, 113), match='[Leslie Howard]'>\n",
      "<_sre.SRE_Match object; span=(190, 227), match='[not that there is much in the novel]'>\n",
      "<_sre.SRE_Match object; span=(272, 285), match='[Bette Davis]'>\n",
      "<_sre.SRE_Match object; span=(1985, 2000), match='[Problem Child]'>\n",
      "<_sre.SRE_Match object; span=(2126, 2132), match='[1996]'>\n",
      "<_sre.SRE_Match object; span=(472, 538), match='[ Love Comes Softly , Loves Enduring Promise & Lo>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='[POSSIBLE SPOILER ALERT]'>\n",
      "<_sre.SRE_Match object; span=(1257, 1263), match='[1966]'>\n",
      "<_sre.SRE_Match object; span=(706, 727), match='[but profoundly dull]'>\n",
      "<_sre.SRE_Match object; span=(0, 62), match='[WARNING: Some spoilers included, though it is a >\n",
      "<_sre.SRE_Match object; span=(100, 115), match='[Spoiler ahead]'>\n",
      "<_sre.SRE_Match object; span=(2005, 2025), match='[%historical legend]'>\n",
      "<_sre.SRE_Match object; span=(115, 169), match='[father: Antoine (1840-1911), sons: Auguste and L>\n",
      "<_sre.SRE_Match object; span=(384, 396), match='[****------]'>\n",
      "<_sre.SRE_Match object; span=(36, 73), match='[Australian Broadcasting Corporation]'>\n",
      "<_sre.SRE_Match object; span=(110, 120), match='[WWII POW]'>\n",
      "<_sre.SRE_Match object; span=(1760, 1764), match='[45]'>\n",
      "<_sre.SRE_Match object; span=(4158, 4162), match='[63]'>\n",
      "<_sre.SRE_Match object; span=(226, 241), match='[Anthony Quinn]'>\n",
      "<_sre.SRE_Match object; span=(1967, 1973), match='[8/10]'>\n",
      "<_sre.SRE_Match object; span=(613, 619), match='[1964]'>\n",
      "<_sre.SRE_Match object; span=(634, 640), match='[1964]'>\n",
      "<_sre.SRE_Match object; span=(1509, 1513), match='[16]'>\n",
      "<_sre.SRE_Match object; span=(2259, 2268), match='[spoiler]'>\n",
      "<_sre.SRE_Match object; span=(1265, 1271), match='[1/10]'>\n",
      "<_sre.SRE_Match object; span=(418, 443), match='[= gay-hating evangelist]'>\n",
      "<_sre.SRE_Match object; span=(3032, 3047), match='[BEGIN SPOILER]'>\n",
      "<_sre.SRE_Match object; span=(3764, 3777), match='[END SPOILER]'>\n",
      "<_sre.SRE_Match object; span=(1640, 1646), match='[1930]'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='[CONTAINS SPOILERS!!!]'>\n",
      "<_sre.SRE_Match object; span=(375, 390), match='[Mark Whalberg]'>\n",
      "<_sre.SRE_Match object; span=(407, 422), match='[Edward Norton]'>\n",
      "<_sre.SRE_Match object; span=(454, 497), match=\"[Seth Green, you'll get the nickname later]\">\n",
      "<_sre.SRE_Match object; span=(512, 527), match='[Jason Statham]'>\n",
      "<_sre.SRE_Match object; span=(538, 548), match='[Mos Deaf]'>\n",
      "<_sre.SRE_Match object; span=(565, 584), match='[Donald Sutherland]'>\n",
      "<_sre.SRE_Match object; span=(1686, 1692), match='[1931]'>\n",
      "<_sre.SRE_Match object; span=(401, 422), match='[\"Grounded For Life\"]'>\n",
      "<_sre.SRE_Match object; span=(146, 152), match='[1972]'>\n",
      "<_sre.SRE_Match object; span=(2097, 2103), match='[1992]'>\n",
      "<_sre.SRE_Match object; span=(4826, 4866), match='[while looking at herself in the mirror]'>\n",
      "<_sre.SRE_Match object; span=(1735, 1798), match=\"[maybe people just can't be bothered to make them>\n",
      "<_sre.SRE_Match object; span=(2357, 2503), match=\"[One can't deny that they had chemistry. Don't ev>\n",
      "<_sre.SRE_Match object; span=(1456, 1473), match='[spoilers follow]'>\n",
      "<_sre.SRE_Match object; span=(954, 958), match='[28]'>\n",
      "<_sre.SRE_Match object; span=(1102, 1106), match='[20]'>\n",
      "<_sre.SRE_Match object; span=(471, 655), match='[Granted, that was Bushie Jr.\\'s whole campaign, >\n",
      "<_sre.SRE_Match object; span=(0, 28), match='[WARNING: CONTAINS SPOILERS]'>\n",
      "<_sre.SRE_Match object; span=(202, 209), match='[rumor]'>\n",
      "<_sre.SRE_Match object; span=(1503, 1511), match='[1953/7]'>\n",
      "<_sre.SRE_Match object; span=(85, 100), match='[Harry Crystal]'>\n",
      "<_sre.SRE_Match object; span=(730, 745), match='[Harry Crystal]'>\n",
      "<_sre.SRE_Match object; span=(243, 249), match='[1973]'>\n",
      "<_sre.SRE_Match object; span=(1297, 1303), match='[1971]'>\n",
      "<_sre.SRE_Match object; span=(1352, 1358), match='[1970]'>\n",
      "<_sre.SRE_Match object; span=(1377, 1383), match='[1971]'>\n",
      "<_sre.SRE_Match object; span=(188, 200), match='[2008-12-21]'>\n",
      "<_sre.SRE_Match object; span=(1308, 1314), match='[1946]'>\n",
      "<_sre.SRE_Match object; span=(1676, 1732), match=\"[It's not what you know, but how you use what you>\n",
      "<_sre.SRE_Match object; span=(268, 277), match='[1967/71]'>\n",
      "<_sre.SRE_Match object; span=(810, 816), match='[1961]'>\n",
      "<_sre.SRE_Match object; span=(314, 335), match='[i.e. \"American Pie\"]'>\n",
      "<_sre.SRE_Match object; span=(403, 438), match=\"[i.e. Julia Roberts' entire career]\">\n",
      "<_sre.SRE_Match object; span=(689, 720), match='[Jay Mohr & Julianne Nicholson]'>\n",
      "<_sre.SRE_Match object; span=(1442, 1509), match='[Ed can not sleep with her mother or, for that ma>\n",
      "<_sre.SRE_Match object; span=(3353, 3373), match='[on a 5 star system]'>\n",
      "<_sre.SRE_Match object; span=(243, 249), match='[wow!]'>\n",
      "<_sre.SRE_Match object; span=(15, 28), match='[Walt Disney]'>\n",
      "<_sre.SRE_Match object; span=(2462, 2466), match='[he]'>\n",
      "<_sre.SRE_Match object; span=(326, 353), match='[in this case, a minor one]'>\n",
      "<_sre.SRE_Match object; span=(635, 649), match='[Gosford Park]'>\n",
      "<_sre.SRE_Match object; span=(0, 13), match='[No Spoilers]'>\n",
      "<_sre.SRE_Match object; span=(1778, 1784), match='[0/10]'>\n",
      "<_sre.SRE_Match object; span=(1785, 1798), match='[6 (1+ - 6-)]'>\n",
      "<_sre.SRE_Match object; span=(1799, 1804), match='[0/4]'>\n",
      "<_sre.SRE_Match object; span=(1619, 1622), match='[1]'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='[***POSSIBLE SPOILERS***]'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='[Minor spoilers follow]'>\n",
      "<_sre.SRE_Match object; span=(445, 451), match='[1958]'>\n",
      "<_sre.SRE_Match object; span=(475, 481), match='[1959]'>\n",
      "<_sre.SRE_Match object; span=(2385, 2388), match='[!]'>\n",
      "<_sre.SRE_Match object; span=(662, 674), match='[Rate: 7/10]'>\n",
      "<_sre.SRE_Match object; span=(194, 244), match='[except Combat Shock...That sucker was depressing>\n",
      "<_sre.SRE_Match object; span=(836, 842), match='[1970]'>\n",
      "<_sre.SRE_Match object; span=(850, 856), match='[1979]'>\n",
      "<_sre.SRE_Match object; span=(303, 309), match='[1953]'>\n",
      "<_sre.SRE_Match object; span=(951, 960), match='[SPOILER]'>\n",
      "<_sre.SRE_Match object; span=(809, 815), match='[1979]'>\n",
      "<_sre.SRE_Match object; span=(190, 228), match=\"[That's the last time that'll happen.]\">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "5        None\n",
       "6        None\n",
       "7        None\n",
       "8        None\n",
       "9        None\n",
       "10       None\n",
       "11       None\n",
       "12       None\n",
       "13       None\n",
       "14       None\n",
       "15       None\n",
       "16       None\n",
       "17       None\n",
       "18       None\n",
       "19       None\n",
       "20       None\n",
       "21       None\n",
       "22       None\n",
       "23       None\n",
       "24       None\n",
       "25       None\n",
       "26       None\n",
       "27       None\n",
       "28       None\n",
       "29       None\n",
       "         ... \n",
       "49970    None\n",
       "49971    None\n",
       "49972    None\n",
       "49973    None\n",
       "49974    None\n",
       "49975    None\n",
       "49976    None\n",
       "49977    None\n",
       "49978    None\n",
       "49979    None\n",
       "49980    None\n",
       "49981    None\n",
       "49982    None\n",
       "49983    None\n",
       "49984    None\n",
       "49985    None\n",
       "49986    None\n",
       "49987    None\n",
       "49988    None\n",
       "49989    None\n",
       "49990    None\n",
       "49991    None\n",
       "49992    None\n",
       "49993    None\n",
       "49994    None\n",
       "49995    None\n",
       "49996    None\n",
       "49997    None\n",
       "49998    None\n",
       "49999    None\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review.apply(find_square_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the square brackets content\n",
    "def remove_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review = df.review.apply(remove_square_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "5        None\n",
       "6        None\n",
       "7        None\n",
       "8        None\n",
       "9        None\n",
       "10       None\n",
       "11       None\n",
       "12       None\n",
       "13       None\n",
       "14       None\n",
       "15       None\n",
       "16       None\n",
       "17       None\n",
       "18       None\n",
       "19       None\n",
       "20       None\n",
       "21       None\n",
       "22       None\n",
       "23       None\n",
       "24       None\n",
       "25       None\n",
       "26       None\n",
       "27       None\n",
       "28       None\n",
       "29       None\n",
       "         ... \n",
       "49970    None\n",
       "49971    None\n",
       "49972    None\n",
       "49973    None\n",
       "49974    None\n",
       "49975    None\n",
       "49976    None\n",
       "49977    None\n",
       "49978    None\n",
       "49979    None\n",
       "49980    None\n",
       "49981    None\n",
       "49982    None\n",
       "49983    None\n",
       "49984    None\n",
       "49985    None\n",
       "49986    None\n",
       "49987    None\n",
       "49988    None\n",
       "49989    None\n",
       "49990    None\n",
       "49991    None\n",
       "49992    None\n",
       "49993    None\n",
       "49994    None\n",
       "49995    None\n",
       "49996    None\n",
       "49997    None\n",
       "49998    None\n",
       "49999    None\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify again\n",
    "df.review.apply(find_square_brackets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove the special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review = df.review.apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One of the other reviewers has mentioned that after watching just 1 Oz episode youll be hooked They are right as this is exactly what happened with meThe first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the wordIt is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Em City is home to manyAryans Muslims gangstas Latinos Christians Italians Irish and moreso scuffles death stares dodgy dealings and shady agreements are never far awayI would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare Forget pretty pictures painted for mainstream audiences forget charm forget romanceOZ doesnt mess around The first episode I ever saw struck me as so nasty it was surreal I couldnt say I was ready for it but as I watched more I developed a taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary nltk modules first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = list(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    token = word_tokenize(text)\n",
    "    text_new = []\n",
    "    for word in token:\n",
    "        if word not in stopword_list:\n",
    "            text_new.append(word)\n",
    "    text = ' '.join(text_new)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One reviewers mentioned watching 1 Oz episode youll hooked They right exactly happened meThe first thing struck Oz brutality unflinching scenes violence set right word GO Trust show faint hearted timid This show pulls punches regards drugs sex violence Its hardcore classic use wordIt called OZ nickname given Oswald Maximum Security State Penitentary It focuses mainly Emerald City experimental section prison cells glass fronts face inwards privacy high agenda Em City home manyAryans Muslims gangstas Latinos Christians Italians Irish moreso scuffles death stares dodgy dealings shady agreements never far awayI would say main appeal show due fact goes shows wouldnt dare Forget pretty pictures painted mainstream audiences forget charm forget romanceOZ doesnt mess around The first episode I ever saw struck nasty surreal I couldnt say I ready I watched I developed taste Oz got accustomed high levels graphic violence Not violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience Watching Oz may become comfortable uncomfortable viewingthats get touch darker side'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review = df.review.apply(remove_stopwords)\n",
    "df.review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stemming of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    pst = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([pst.stem(word) for word in text.split()])\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review = df.review.apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one review mention watch 1 Oz episod youll hook they right exactli happen meth first thing struck Oz brutal unflinch scene violenc set right word GO trust show faint heart timid thi show pull punch regard drug sex violenc it hardcor classic use wordit call OZ nicknam given oswald maximum secur state penitentari It focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda Em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around the first episod I ever saw struck nasti surreal I couldnt say I readi I watch I develop tast Oz got accustom high level graphic violenc not violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch Oz may becom comfort uncomfort viewingthat get touch darker side'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one review mention watch 1 Oz episod youll hoo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonder littl product the film techniqu unass...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought wonder way spend time hot summer wee...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic there famili littl boy jake think there ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one review mention watch 1 Oz episod youll hoo...  positive\n",
       "1  A wonder littl product the film techniqu unass...  positive\n",
       "2  I thought wonder way spend time hot summer wee...  positive\n",
       "3  basic there famili littl boy jake think there ...  negative\n",
       "4  petter mattei love time money visual stun film...  positive"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df[df['sentiment']=='positive']\n",
    "df_neg = df[df['sentiment']=='negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling the dataset - 5k pos and 5k negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample5_pos = df_pos.sample(5000, random_state=0)\n",
    "sample5_neg = df_neg.sample(5000, random_state=0)\n",
    "sample5_df = sample5_pos.append(sample5_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data preparation\n",
    "sample5_indexes = sample5_df.index.values.tolist()\n",
    "main_indexes = df.index.values.tolist()\n",
    "test5_df = pd.DataFrame(columns=['review','sentiment'])\n",
    "for i in main_indexes:\n",
    "    if i not in sample5_indexes:\n",
    "        test5_df = test5_df.append(df[df.index==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample5_df = sample5_df.reset_index().drop('index',axis=1)\n",
    "test5_df = test5_df.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n",
      "(40000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(sample5_df.shape)\n",
    "print(test5_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW train CV: (10000, 1860522)\n",
      "BOW test CV: (40000, 1860522)\n",
      "Tfidf train: (10000, 1860522)\n",
      "Tfidf test: (40000, 1860522)\n"
     ]
    }
   ],
   "source": [
    "s5norm_train_reviews = sample5_df.review\n",
    "s5norm_test_reviews = test5_df.review\n",
    "\n",
    "#Count vectorizer for bag of words\n",
    "s5cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "\n",
    "#transformed reviews\n",
    "s5cv_train_reviews=s5cv.fit_transform(s5norm_train_reviews)\n",
    "s5cv_test_reviews=s5cv.transform(s5norm_test_reviews)\n",
    "\n",
    "print('BOW train CV:',s5cv_train_reviews.shape)\n",
    "print('BOW test CV:',s5cv_test_reviews.shape)\n",
    "\n",
    "#Tfidf vectorizer\n",
    "s5tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "\n",
    "#transformed reviews\n",
    "s5tv_train_reviews=s5tv.fit_transform(s5norm_train_reviews)\n",
    "s5tv_test_reviews=s5tv.transform(s5norm_test_reviews)\n",
    "\n",
    "print('Tfidf train:',s5tv_train_reviews.shape)\n",
    "print('Tfidf test:',s5tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n",
      "(40000, 1)\n"
     ]
    }
   ],
   "source": [
    "#labeling the sentient data\n",
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "s5sentiment_train_data=lb.fit_transform(sample5_df['sentiment'])\n",
    "s5sentiment_test_data=lb.fit_transform(test5_df['sentiment'])\n",
    "print(s5sentiment_train_data.shape)\n",
    "print(s5sentiment_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=500, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=500, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "[0 1 1 ... 0 0 1]\n",
      "[0 1 1 ... 0 0 1]\n",
      "lr_bow_score : 0.7184\n",
      "lr_tfidf_score : 0.71895\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "s5lr=LogisticRegression(penalty='l2',max_iter=500,C=10,random_state=42)\n",
    "#Fitting the model for Bag of words\n",
    "s5lr_bow=s5lr.fit(s5cv_train_reviews, s5sentiment_train_data)\n",
    "print(s5lr_bow)\n",
    "#Fitting the model for tfidf features\n",
    "s5lr_tfidf=s5lr.fit(s5tv_train_reviews, s5sentiment_train_data)\n",
    "print(s5lr_tfidf)\n",
    "\n",
    "##Predicting the model for BOW features\n",
    "s5lr_bow_predict=s5lr.predict(s5cv_test_reviews)\n",
    "print(s5lr_bow_predict)\n",
    "##Predicting the model for tfidf features\n",
    "s5lr_tfidf_predict=s5lr.predict(s5tv_test_reviews)\n",
    "print(s5lr_tfidf_predict)\n",
    "\n",
    "#Accuracy score for bag of words\n",
    "s5lr_bow_score=accuracy_score(s5sentiment_test_data, s5lr_bow_predict)\n",
    "print(\"lr_bow_score :\",s5lr_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "s5lr_tfidf_score=accuracy_score(s5sentiment_test_data, s5lr_tfidf_predict)\n",
    "print(\"lr_tfidf_score :\",s5lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB BOW Score:  0.719\n",
      "Multinomial NB TF-IDF Score:  0.719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "# MultinomialNB\n",
    "s5MNB = MultinomialNB()\n",
    "\n",
    "s5MNB_bow = s5MNB.fit(s5cv_train_reviews, s5sentiment_train_data)\n",
    "s5MNB_tfidf = s5MNB.fit(s5tv_train_reviews, s5sentiment_train_data)\n",
    "\n",
    "s5MNB_bow_predict = s5MNB.predict(s5cv_test_reviews)\n",
    "s5MNB_bow_score = accuracy_score(s5sentiment_test_data, s5MNB_bow_predict)\n",
    "\n",
    "s5MNB_tfidf_predict = s5MNB.predict(s5tv_test_reviews)\n",
    "s5MNB_tfidf_score = accuracy_score(s5sentiment_test_data, s5MNB_tfidf_predict)\n",
    "\n",
    "print(\"Multinomial NB BOW Score: \", s5MNB_bow_score)\n",
    "print(\"Multinomial NB TF-IDF Score: \", s5MNB_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for poly SVC for BOW model is:  0.68145\n",
      "The accuracy score for poly SVC for TFIDF model is:  0.68145\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "classifier5_poly = svm.SVC(kernel='poly')\n",
    "\n",
    "classifier5_poly.fit(s5cv_train_reviews, s5sentiment_train_data)\n",
    "classifier5_poly.fit(s5tv_train_reviews, s5sentiment_train_data)\n",
    "\n",
    "svc5_bow_pred = classifier5_poly.predict(s5cv_test_reviews)\n",
    "svc5_tfidf_pred = classifier5_poly.predict(s5tv_test_reviews)\n",
    "\n",
    "svc5_bow_score = accuracy_score(s5sentiment_test_data, svc5_bow_pred)\n",
    "svc5_tfidf_score = accuracy_score(s5sentiment_test_data, svc5_tfidf_pred)\n",
    "\n",
    "print(\"The accuracy score for poly SVC for BOW model is: \", svc5_bow_score)\n",
    "print(\"The accuracy score for poly SVC for TFIDF model is: \", svc5_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for linear SVC for BOW model is:  0.718175\n",
      "The accuracy score for linear SVC for TFIDF model is:  0.718175\n"
     ]
    }
   ],
   "source": [
    "classifier5_linear = svm.SVC(kernel='linear')\n",
    "\n",
    "classifier5_linear.fit(s5cv_train_reviews, s5sentiment_train_data)\n",
    "classifier5_linear.fit(s5tv_train_reviews, s5sentiment_train_data)\n",
    "\n",
    "svc5_bow_pred1 = classifier5_linear.predict(s5cv_test_reviews)\n",
    "svc5_tfidf_pred1 = classifier5_linear.predict(s5tv_test_reviews)\n",
    "\n",
    "svc5_bow_score1 = accuracy_score(s5sentiment_test_data, svc5_bow_pred1)\n",
    "svc5_tfidf_score1 = accuracy_score(s5sentiment_test_data, svc5_tfidf_pred1)\n",
    "\n",
    "print(\"The accuracy score for linear SVC for BOW model is: \", svc5_bow_score1)\n",
    "print(\"The accuracy score for linear SVC for TFIDF model is: \", svc5_tfidf_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for rbf SVC for BOW model is:  0.71825\n",
      "The accuracy score for rbf SVC for TFIDF model is:  0.71735\n"
     ]
    }
   ],
   "source": [
    "classifier5_rbf = svm.SVC(kernel='rbf')\n",
    "\n",
    "classifier5_rbf.fit(s5cv_train_reviews, s5sentiment_train_data)\n",
    "classifier5_rbf.fit(s5tv_train_reviews, s5sentiment_train_data)\n",
    "\n",
    "svc5_rbf_bow_pred = classifier5_rbf.predict(s5cv_test_reviews)\n",
    "svc5_rbf_tfidf_pred = classifier5_rbf.predict(s5tv_test_reviews)\n",
    "\n",
    "svc5_rbf_bow_score = accuracy_score(s5sentiment_test_data, svc5_rbf_bow_pred)\n",
    "svc5_rbf_tfidf_score = accuracy_score(s5sentiment_test_data, svc5_rbf_tfidf_pred)\n",
    "\n",
    "print(\"The accuracy score for rbf SVC for BOW model is: \", svc5_rbf_bow_score)\n",
    "print(\"The accuracy score for rbf SVC for TFIDF model is: \", svc5_rbf_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling the dataset - 10k pos and 10k negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample10_pos = df_pos.sample(10000, random_state=0)\n",
    "sample10_neg = df_neg.sample(10000, random_state=0)\n",
    "sample10_df = sample10_pos.append(sample10_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data preparation\n",
    "sample10_indexes = sample10_df.index.values.tolist()\n",
    "main_indexes = df.index.values.tolist()\n",
    "test10_df = pd.DataFrame(columns=['review','sentiment'])\n",
    "for i in main_indexes:\n",
    "    if i not in sample10_indexes:\n",
    "        test10_df = test10_df.append(df[df.index==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample10_df = sample10_df.reset_index().drop('index',axis=1)\n",
    "test10_df = test10_df.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n",
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(sample10_df.shape)\n",
    "print(test10_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW train CV: (20000, 3447884)\n",
      "BOW test CV: (30000, 3447884)\n",
      "Tfidf train: (20000, 3447884)\n",
      "Tfidf test: (30000, 3447884)\n"
     ]
    }
   ],
   "source": [
    "s10norm_train_reviews = sample10_df.review\n",
    "s10norm_test_reviews = test10_df.review\n",
    "\n",
    "#Count vectorizer for bag of words\n",
    "s10cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "\n",
    "#transformed reviews\n",
    "s10cv_train_reviews=s10cv.fit_transform(s10norm_train_reviews)\n",
    "s10cv_test_reviews=s10cv.transform(s10norm_test_reviews)\n",
    "\n",
    "print('BOW train CV:',s10cv_train_reviews.shape)\n",
    "print('BOW test CV:',s10cv_test_reviews.shape)\n",
    "\n",
    "#Tfidf vectorizer\n",
    "s10tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "\n",
    "#transformed reviews\n",
    "s10tv_train_reviews=s10tv.fit_transform(s10norm_train_reviews)\n",
    "s10tv_test_reviews=s10tv.transform(s10norm_test_reviews)\n",
    "\n",
    "print('Tfidf train:',s10tv_train_reviews.shape)\n",
    "print('Tfidf test:',s10tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1)\n",
      "(30000, 1)\n"
     ]
    }
   ],
   "source": [
    "#labeling the sentient data\n",
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "s10sentiment_train_data=lb.fit_transform(sample10_df['sentiment'])\n",
    "s10sentiment_test_data=lb.fit_transform(test10_df['sentiment'])\n",
    "print(s10sentiment_train_data.shape)\n",
    "print(s10sentiment_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=500, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=500, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "[0 1 0 ... 1 1 0]\n",
      "[0 1 0 ... 1 1 0]\n",
      "lr_bow_score : 0.7347\n",
      "lr_tfidf_score : 0.7342\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "s10lr=LogisticRegression(penalty='l2',max_iter=500,C=10,random_state=42)\n",
    "#Fitting the model for Bag of words\n",
    "s10lr_bow=s10lr.fit(s10cv_train_reviews, s10sentiment_train_data)\n",
    "print(s10lr_bow)\n",
    "#Fitting the model for tfidf features\n",
    "s10lr_tfidf=s10lr.fit(s10tv_train_reviews, s10sentiment_train_data)\n",
    "print(s10lr_tfidf)\n",
    "\n",
    "##Predicting the model for BOW features\n",
    "s10lr_bow_predict=s10lr.predict(s10cv_test_reviews)\n",
    "print(s10lr_bow_predict)\n",
    "##Predicting the model for tfidf features\n",
    "s10lr_tfidf_predict=s10lr.predict(s10tv_test_reviews)\n",
    "print(s10lr_tfidf_predict)\n",
    "\n",
    "#Accuracy score for bag of words\n",
    "s10lr_bow_score=accuracy_score(s10sentiment_test_data, s10lr_bow_predict)\n",
    "print(\"lr_bow_score :\",s10lr_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "s10lr_tfidf_score=accuracy_score(s10sentiment_test_data, s10lr_tfidf_predict)\n",
    "print(\"lr_tfidf_score :\",s10lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB BOW Score:  0.7353333333333333\n",
      "Multinomial NB TF-IDF Score:  0.7353333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "# MultinomialNB\n",
    "s10MNB = MultinomialNB()\n",
    "\n",
    "s10MNB_bow = s10MNB.fit(s10cv_train_reviews, s10sentiment_train_data)\n",
    "s10MNB_tfidf = s10MNB.fit(s10tv_train_reviews, s10sentiment_train_data)\n",
    "\n",
    "s10MNB_bow_predict = s10MNB.predict(s10cv_test_reviews)\n",
    "s10MNB_bow_score = accuracy_score(s10sentiment_test_data, s10MNB_bow_predict)\n",
    "\n",
    "s10MNB_tfidf_predict = s10MNB.predict(s10tv_test_reviews)\n",
    "s10MNB_tfidf_score = accuracy_score(s10sentiment_test_data, s10MNB_tfidf_predict)\n",
    "\n",
    "print(\"Multinomial NB BOW Score: \", s10MNB_bow_score)\n",
    "print(\"Multinomial NB TF-IDF Score: \", s10MNB_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for poly SVC for BOW model is:  0.6944333333333333\n",
      "The accuracy score for poly SVC for TFIDF model is:  0.6944333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "classifier10_poly = svm.SVC(kernel='poly')\n",
    "\n",
    "classifier10_poly.fit(s10cv_train_reviews, s10sentiment_train_data)\n",
    "classifier10_poly.fit(s10tv_train_reviews, s10sentiment_train_data)\n",
    "\n",
    "svc10_bow_pred = classifier10_poly.predict(s10cv_test_reviews)\n",
    "svc10_tfidf_pred = classifier10_poly.predict(s10tv_test_reviews)\n",
    "\n",
    "svc10_bow_score = accuracy_score(s10sentiment_test_data, svc10_bow_pred)\n",
    "svc10_tfidf_score = accuracy_score(s10sentiment_test_data, svc10_tfidf_pred)\n",
    "\n",
    "print(\"The accuracy score for poly SVC for BOW model is: \", svc10_bow_score)\n",
    "print(\"The accuracy score for poly SVC for TFIDF model is: \", svc10_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for linear SVC for BOW model is:  0.718175\n",
      "The accuracy score for linear SVC for TFIDF model is:  0.718175\n"
     ]
    }
   ],
   "source": [
    "classifier10_linear = svm.SVC(kernel='linear')\n",
    "\n",
    "classifier10_linear.fit(s10cv_train_reviews, s10sentiment_train_data)\n",
    "classifier10_linear.fit(s10tv_train_reviews, s10sentiment_train_data)\n",
    "\n",
    "svc10_bow_pred1 = classifier10_linear.predict(s10cv_test_reviews)\n",
    "svc10_tfidf_pred1 = classifier10_linear.predict(s10tv_test_reviews)\n",
    "\n",
    "svc10_bow_score1 = accuracy_score(s5sentiment_test_data, svc5_bow_pred1)\n",
    "svc10_tfidf_score1 = accuracy_score(s5sentiment_test_data, svc5_tfidf_pred1)\n",
    "\n",
    "print(\"The accuracy score for linear SVC for BOW model is: \", svc10_bow_score1)\n",
    "print(\"The accuracy score for linear SVC for TFIDF model is: \", svc10_tfidf_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for rbf SVC for BOW model is:  0.5418333333333333\n",
      "The accuracy score for rbf SVC for TFIDF model is:  0.5047666666666667\n"
     ]
    }
   ],
   "source": [
    "classifier10_rbf = svm.SVC(kernel='rbf')\n",
    "\n",
    "classifier10_rbf.fit(s10cv_train_reviews, s10sentiment_train_data)\n",
    "classifier10_rbf.fit(s10tv_train_reviews, s10sentiment_train_data)\n",
    "\n",
    "svc10_rbf_bow_pred = classifier10_rbf.predict(s10cv_test_reviews)\n",
    "svc10_rbf_tfidf_pred = classifier10_rbf.predict(s10tv_test_reviews)\n",
    "\n",
    "svc10_rbf_bow_score = accuracy_score(s10sentiment_test_data, svc10_rbf_bow_pred)\n",
    "svc10_rbf_tfidf_score = accuracy_score(s10sentiment_test_data, svc10_rbf_tfidf_pred)\n",
    "\n",
    "print(\"The accuracy score for rbf SVC for BOW model is: \", svc10_rbf_bow_score)\n",
    "print(\"The accuracy score for rbf SVC for TFIDF model is: \", svc10_rbf_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling 15k postive and 15k negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample15_pos = df_pos.sample(15000, random_state=0)\n",
    "sample15_neg = df_neg.sample(15000, random_state=0)\n",
    "sample15_df = sample15_pos.append(sample15_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data preparation\n",
    "sample15_indexes = sample15_df.index.values.tolist()\n",
    "main_indexes = df.index.values.tolist()\n",
    "test15_df = pd.DataFrame(columns=['review','sentiment'])\n",
    "for i in main_indexes:\n",
    "    if i not in sample15_indexes:\n",
    "        test15_df = test15_df.append(df[df.index==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample15_df = sample15_df.reset_index().drop('index',axis=1)\n",
    "test15_df = test15_df.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 2)\n",
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(sample15_df.shape)\n",
    "print(test15_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW train CV: (30000, 4906126)\n",
      "BOW test CV: (20000, 4906126)\n",
      "Tfidf train: (30000, 4906126)\n",
      "Tfidf test: (20000, 4906126)\n"
     ]
    }
   ],
   "source": [
    "s15norm_train_reviews = sample15_df.review\n",
    "s15norm_test_reviews = test15_df.review\n",
    "\n",
    "#Count vectorizer for bag of words\n",
    "s15cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "\n",
    "#transformed reviews\n",
    "s15cv_train_reviews=s15cv.fit_transform(s15norm_train_reviews)\n",
    "s15cv_test_reviews=s15cv.transform(s15norm_test_reviews)\n",
    "\n",
    "print('BOW train CV:',s15cv_train_reviews.shape)\n",
    "print('BOW test CV:',s15cv_test_reviews.shape)\n",
    "\n",
    "#Tfidf vectorizer\n",
    "s15tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "\n",
    "#transformed reviews\n",
    "s15tv_train_reviews=s15tv.fit_transform(s15norm_train_reviews)\n",
    "s15tv_test_reviews=s15tv.transform(s15norm_test_reviews)\n",
    "\n",
    "print('Tfidf train:',s15tv_train_reviews.shape)\n",
    "print('Tfidf test:',s15tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1)\n",
      "(20000, 1)\n"
     ]
    }
   ],
   "source": [
    "#labeling the sentient data\n",
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "s15sentiment_train_data=lb.fit_transform(sample15_df['sentiment'])\n",
    "s15sentiment_test_data=lb.fit_transform(test15_df['sentiment'])\n",
    "print(s15sentiment_train_data.shape)\n",
    "print(s15sentiment_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=500, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=500, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "[1 0 1 ... 1 0 0]\n",
      "[1 0 1 ... 1 0 0]\n",
      "lr_bow_score : 0.73845\n",
      "lr_tfidf_score : 0.7359\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "s15lr=LogisticRegression(penalty='l2',max_iter=500,C=10,random_state=42)\n",
    "#Fitting the model for Bag of words\n",
    "s15lr_bow=s15lr.fit(s15cv_train_reviews, s15sentiment_train_data)\n",
    "print(s15lr_bow)\n",
    "#Fitting the model for tfidf features\n",
    "s15lr_tfidf=s15lr.fit(s15tv_train_reviews, s15sentiment_train_data)\n",
    "print(s15lr_tfidf)\n",
    "\n",
    "##Predicting the model for BOW features\n",
    "s15lr_bow_predict=s15lr.predict(s15cv_test_reviews)\n",
    "print(s15lr_bow_predict)\n",
    "##Predicting the model for tfidf features\n",
    "s15lr_tfidf_predict=s15lr.predict(s15tv_test_reviews)\n",
    "print(s15lr_tfidf_predict)\n",
    "\n",
    "#Accuracy score for bag of words\n",
    "s15lr_bow_score=accuracy_score(s15sentiment_test_data, s15lr_bow_predict)\n",
    "print(\"lr_bow_score :\",s15lr_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "s15lr_tfidf_score=accuracy_score(s15sentiment_test_data, s15lr_tfidf_predict)\n",
    "print(\"lr_tfidf_score :\",s15lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB BOW Score:  0.7388\n",
      "Multinomial NB TF-IDF Score:  0.7388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "# MultinomialNB\n",
    "s15MNB = MultinomialNB()\n",
    "\n",
    "s15MNB_bow = s15MNB.fit(s15cv_train_reviews, s15sentiment_train_data)\n",
    "s15MNB_tfidf = s15MNB.fit(s15tv_train_reviews, s15sentiment_train_data)\n",
    "\n",
    "s15MNB_bow_predict = s15MNB.predict(s15cv_test_reviews)\n",
    "s15MNB_bow_score = accuracy_score(s15sentiment_test_data, s15MNB_bow_predict)\n",
    "\n",
    "s15MNB_tfidf_predict = s15MNB.predict(s15tv_test_reviews)\n",
    "s15MNB_tfidf_score = accuracy_score(s15sentiment_test_data, s15MNB_tfidf_predict)\n",
    "\n",
    "print(\"Multinomial NB BOW Score: \", s15MNB_bow_score)\n",
    "print(\"Multinomial NB TF-IDF Score: \", s15MNB_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for poly SVC for BOW model is:  0.7023\n",
      "The accuracy score for poly SVC for TFIDF model is:  0.7023\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "classifier15_poly = svm.SVC(kernel='poly')\n",
    "\n",
    "classifier15_poly.fit(s15cv_train_reviews, s15sentiment_train_data)\n",
    "classifier15_poly.fit(s15tv_train_reviews, s15sentiment_train_data)\n",
    "\n",
    "svc15_bow_pred = classifier15_poly.predict(s15cv_test_reviews)\n",
    "svc15_tfidf_pred = classifier15_poly.predict(s15tv_test_reviews)\n",
    "\n",
    "svc15_bow_score = accuracy_score(s15sentiment_test_data, svc15_bow_pred)\n",
    "svc15_tfidf_score = accuracy_score(s15sentiment_test_data, svc15_tfidf_pred)\n",
    "\n",
    "print(\"The accuracy score for poly SVC for BOW model is: \", svc15_bow_score)\n",
    "print(\"The accuracy score for poly SVC for TFIDF model is: \", svc15_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for linear SVC for BOW model is:  0.73915\n",
      "The accuracy score for linear SVC for TFIDF model is:  0.73915\n"
     ]
    }
   ],
   "source": [
    "classifier15_linear = svm.SVC(kernel='linear')\n",
    "\n",
    "classifier15_linear.fit(s15cv_train_reviews, s15sentiment_train_data)\n",
    "classifier15_linear.fit(s15tv_train_reviews, s15sentiment_train_data)\n",
    "\n",
    "svc15_bow_pred1 = classifier15_linear.predict(s15cv_test_reviews)\n",
    "svc15_tfidf_pred1 = classifier15_linear.predict(s15tv_test_reviews)\n",
    "\n",
    "svc15_bow_score1 = accuracy_score(s15sentiment_test_data, svc15_bow_pred1)\n",
    "svc15_tfidf_score1 = accuracy_score(s15sentiment_test_data, svc15_tfidf_pred1)\n",
    "\n",
    "print(\"The accuracy score for linear SVC for BOW model is: \", svc15_bow_score1)\n",
    "print(\"The accuracy score for linear SVC for TFIDF model is: \", svc15_tfidf_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for rbf SVC for BOW model is:  0.51035\n",
      "The accuracy score for rbf SVC for TFIDF model is:  0.5\n"
     ]
    }
   ],
   "source": [
    "classifier15_rbf = svm.SVC(kernel='rbf')\n",
    "\n",
    "classifier15_rbf.fit(s15cv_train_reviews, s15sentiment_train_data)\n",
    "classifier15_rbf.fit(s15tv_train_reviews, s15sentiment_train_data)\n",
    "\n",
    "svc15_rbf_bow_pred = classifier15_rbf.predict(s15cv_test_reviews)\n",
    "svc15_rbf_tfidf_pred = classifier15_rbf.predict(s15tv_test_reviews)\n",
    "\n",
    "svc15_rbf_bow_score = accuracy_score(s15sentiment_test_data, svc15_rbf_bow_pred)\n",
    "svc15_rbf_tfidf_score = accuracy_score(s15sentiment_test_data, svc15_rbf_tfidf_pred)\n",
    "\n",
    "print(\"The accuracy score for rbf SVC for BOW model is: \", svc15_rbf_bow_score)\n",
    "print(\"The accuracy score for rbf SVC for TFIDF model is: \", svc15_rbf_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Accuracy (%)\n",
    "#### Bag of Words\n",
    "\n",
    "| Algorithm          | 10k    | 20k    | 30k    |\n",
    "|--------------------|------------|-------------|------------|\n",
    "| Logistic Regression                | 71.84       | 73.47        | 73.84         |\n",
    "| Multinomial NB      | 71.90       | 73.53        |   73.88       |\n",
    "| SVM - Linear | 71.81       | 71.81        | 73.91       |\n",
    "| SVM - Poly | 68.14       | 69.40        | 70.23       |\n",
    "| SVM - RBF | 71.82       | 54.18        | 51.03       |\n",
    "\n",
    "#### TFIDF\n",
    "| Algorithm          | 10k    | 20k    | 30k    |\n",
    "|--------------------|------------|-------------|------------|\n",
    "| Logistic Regression                | 71.89       | 73.42        | 73.59         |\n",
    "| Multinomial NB      | 71.90       | 73.53        | 73.88         |\n",
    "| SVM - Linear | 71.81       | 71.81        | 73.91       |\n",
    "| SVM - Poly | 68.14       | 69.40        | 70.23       |\n",
    "| SVM - RBF | 71.73       | 50.47        | 50.00       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Classification Report - Precision, Recall and F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10K Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.72      0.72      0.72     20000\n",
      "   Negative       0.72      0.72      0.72     20000\n",
      "\n",
      "avg / total       0.72      0.72      0.72     40000\n",
      "\n",
      "Logistic Regression - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.72      0.72      0.72     20000\n",
      "   Negative       0.72      0.71      0.72     20000\n",
      "\n",
      "avg / total       0.72      0.72      0.72     40000\n",
      "\n",
      "Multinomial Naives Bayes - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.72      0.72      0.72     20000\n",
      "   Negative       0.72      0.71      0.72     20000\n",
      "\n",
      "avg / total       0.72      0.72      0.72     40000\n",
      "\n",
      "Multinomial Naives Bayes - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.72      0.72      0.72     20000\n",
      "   Negative       0.72      0.71      0.72     20000\n",
      "\n",
      "avg / total       0.72      0.72      0.72     40000\n",
      "\n",
      "SVM kernel='poly' - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.69      0.67      0.68     20000\n",
      "   Negative       0.68      0.69      0.68     20000\n",
      "\n",
      "avg / total       0.68      0.68      0.68     40000\n",
      "\n",
      "SVM kernel='poly' - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.69      0.67      0.68     20000\n",
      "   Negative       0.68      0.69      0.68     20000\n",
      "\n",
      "avg / total       0.68      0.68      0.68     40000\n",
      "\n",
      "SVM kernel='linear' - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.72      0.71      0.72     20000\n",
      "   Negative       0.72      0.72      0.72     20000\n",
      "\n",
      "avg / total       0.72      0.72      0.72     40000\n",
      "\n",
      "SVM kernel='linear' - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.72      0.71      0.72     20000\n",
      "   Negative       0.72      0.72      0.72     20000\n",
      "\n",
      "avg / total       0.72      0.72      0.72     40000\n",
      "\n",
      "SVM kernel='rbf' - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.72      0.72      0.72     20000\n",
      "   Negative       0.72      0.71      0.72     20000\n",
      "\n",
      "avg / total       0.72      0.72      0.72     40000\n",
      "\n",
      "SVM kernel='rbf' - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.71      0.75      0.73     20000\n",
      "   Negative       0.73      0.69      0.71     20000\n",
      "\n",
      "avg / total       0.72      0.72      0.72     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "lr5_bow_report=classification_report(s5sentiment_test_data,s5lr_bow_predict,target_names=['Positive','Negative'])\n",
    "lr5_tfidf_report=classification_report(s5sentiment_test_data,s5lr_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(\"Logistic Regression - Bag of Words: \\n\",lr5_bow_report)\n",
    "print(\"Logistic Regression - TFIDF: \\n\",lr5_tfidf_report)\n",
    "\n",
    "#Multinomial Naives Bayes\n",
    "mnb5_bow_report=classification_report(s5sentiment_test_data,s5MNB_bow_predict,target_names=['Positive','Negative'])\n",
    "mnb5_tfidf_report=classification_report(s5sentiment_test_data,s5MNB_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(\"Multinomial Naives Bayes - Bag of Words: \\n\",mnb5_bow_report)\n",
    "print(\"Multinomial Naives Bayes - TFIDF: \\n\",mnb5_tfidf_report)\n",
    "\n",
    "\n",
    "#Support Vector Machines - Poly\n",
    "svc5_poly_bow_report=classification_report(s5sentiment_test_data,svc5_bow_pred,target_names=['Positive','Negative'])\n",
    "svc5_poly_tfidf_report=classification_report(s5sentiment_test_data,svc5_tfidf_pred,target_names=['Positive','Negative'])\n",
    "print(\"SVM kernel='poly' - Bag of Words: \\n\",svc5_poly_bow_report)\n",
    "print(\"SVM kernel='poly' - TFIDF: \\n\",svc5_poly_tfidf_report)\n",
    "\n",
    "#Support Vector Machines - Linear\n",
    "svc5_linear_bow_report=classification_report(s5sentiment_test_data,svc5_bow_pred1,target_names=['Positive','Negative'])\n",
    "svc5_linear_tfidf_report=classification_report(s5sentiment_test_data,svc5_tfidf_pred1,target_names=['Positive','Negative'])\n",
    "print(\"SVM kernel='linear' - Bag of Words: \\n\",svc5_linear_bow_report)\n",
    "print(\"SVM kernel='linear' - TFIDF: \\n\",svc5_linear_tfidf_report)\n",
    "\n",
    "#Support Vector Machines - rbf\n",
    "svc5_rbf_bow_report=classification_report(s5sentiment_test_data,svc5_rbf_bow_pred,target_names=['Positive','Negative'])\n",
    "svc5_rbf_tfidf_report=classification_report(s5sentiment_test_data,svc5_rbf_tfidf_pred,target_names=['Positive','Negative'])\n",
    "print(\"SVM kernel='rbf' - Bag of Words: \\n\",svc5_rbf_bow_report)\n",
    "print(\"SVM kernel='rbf' - TFIDF: \\n\",svc5_rbf_tfidf_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20K Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.74      0.73      0.73     15000\n",
      "   Negative       0.73      0.74      0.74     15000\n",
      "\n",
      "avg / total       0.73      0.73      0.73     30000\n",
      "\n",
      "Logistic Regression - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.72      0.76      0.74     15000\n",
      "   Negative       0.75      0.71      0.73     15000\n",
      "\n",
      "avg / total       0.73      0.73      0.73     30000\n",
      "\n",
      "Multinomial Naives Bayes - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.73      0.74      0.74     15000\n",
      "   Negative       0.74      0.73      0.73     15000\n",
      "\n",
      "avg / total       0.74      0.74      0.74     30000\n",
      "\n",
      "Multinomial Naives Bayes - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.73      0.74      0.74     15000\n",
      "   Negative       0.74      0.73      0.73     15000\n",
      "\n",
      "avg / total       0.74      0.74      0.74     30000\n",
      "\n",
      "SVM kernel='poly' - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.70      0.68      0.69     15000\n",
      "   Negative       0.69      0.71      0.70     15000\n",
      "\n",
      "avg / total       0.69      0.69      0.69     30000\n",
      "\n",
      "SVM kernel='poly' - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.70      0.68      0.69     15000\n",
      "   Negative       0.69      0.71      0.70     15000\n",
      "\n",
      "avg / total       0.69      0.69      0.69     30000\n",
      "\n",
      "SVM kernel='linear' - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.74      0.73      0.73     15000\n",
      "   Negative       0.73      0.74      0.74     15000\n",
      "\n",
      "avg / total       0.73      0.73      0.73     30000\n",
      "\n",
      "SVM kernel='linear' - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.74      0.73      0.73     15000\n",
      "   Negative       0.73      0.74      0.74     15000\n",
      "\n",
      "avg / total       0.73      0.73      0.73     30000\n",
      "\n",
      "SVM kernel='rbf' - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.94      0.09      0.16     15000\n",
      "   Negative       0.52      0.99      0.68     15000\n",
      "\n",
      "avg / total       0.73      0.54      0.42     30000\n",
      "\n",
      "SVM kernel='rbf' - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.99      0.01      0.02     15000\n",
      "   Negative       0.50      1.00      0.67     15000\n",
      "\n",
      "avg / total       0.75      0.50      0.34     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "lr10_bow_report=classification_report(s10sentiment_test_data,s10lr_bow_predict,target_names=['Positive','Negative'])\n",
    "lr10_tfidf_report=classification_report(s10sentiment_test_data,s10lr_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(\"Logistic Regression - Bag of Words: \\n\",lr10_bow_report)\n",
    "print(\"Logistic Regression - TFIDF: \\n\",lr10_tfidf_report)\n",
    "\n",
    "#Multinomial Naives Bayes\n",
    "mnb10_bow_report=classification_report(s10sentiment_test_data,s10MNB_bow_predict,target_names=['Positive','Negative'])\n",
    "mnb10_tfidf_report=classification_report(s10sentiment_test_data,s10MNB_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(\"Multinomial Naives Bayes - Bag of Words: \\n\",mnb10_bow_report)\n",
    "print(\"Multinomial Naives Bayes - TFIDF: \\n\",mnb10_tfidf_report)\n",
    "\n",
    "#Support Vector Machines - Poly\n",
    "svc10_poly_bow_report=classification_report(s10sentiment_test_data,svc10_bow_pred,target_names=['Positive','Negative'])\n",
    "svc10_poly_tfidf_report=classification_report(s10sentiment_test_data,svc10_tfidf_pred,target_names=['Positive','Negative'])\n",
    "print(\"SVM kernel='poly' - Bag of Words: \\n\",svc10_poly_bow_report)\n",
    "print(\"SVM kernel='poly' - TFIDF: \\n\",svc10_poly_tfidf_report)\n",
    "\n",
    "#Support Vector Machines - Linear\n",
    "svc10_linear_bow_report=classification_report(s10sentiment_test_data,svc10_bow_pred1,target_names=['Positive','Negative'])\n",
    "svc10_linear_tfidf_report=classification_report(s10sentiment_test_data,svc10_tfidf_pred1,target_names=['Positive','Negative'])\n",
    "print(\"SVM kernel='linear' - Bag of Words: \\n\",svc10_linear_bow_report)\n",
    "print(\"SVM kernel='linear' - TFIDF: \\n\",svc10_linear_tfidf_report)\n",
    "\n",
    "#Support Vector Machines - rbf\n",
    "svc10_rbf_bow_report=classification_report(s10sentiment_test_data,svc10_rbf_bow_pred,target_names=['Positive','Negative'])\n",
    "svc10_rbf_tfidf_report=classification_report(s10sentiment_test_data,svc10_rbf_tfidf_pred,target_names=['Positive','Negative'])\n",
    "print(\"SVM kernel='rbf' - Bag of Words: \\n\",svc10_rbf_bow_report)\n",
    "print(\"SVM kernel='rbf' - TFIDF: \\n\",svc10_rbf_tfidf_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30K Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.74      0.74      0.74     10000\n",
      "   Negative       0.74      0.74      0.74     10000\n",
      "\n",
      "avg / total       0.74      0.74      0.74     20000\n",
      "\n",
      "Logistic Regression - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.72      0.77      0.75     10000\n",
      "   Negative       0.75      0.70      0.73     10000\n",
      "\n",
      "avg / total       0.74      0.74      0.74     20000\n",
      "\n",
      "Multinomial Naives Bayes - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.73      0.75      0.74     10000\n",
      "   Negative       0.74      0.73      0.74     10000\n",
      "\n",
      "avg / total       0.74      0.74      0.74     20000\n",
      "\n",
      "Multinomial Naives Bayes - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.73      0.75      0.74     10000\n",
      "   Negative       0.74      0.73      0.74     10000\n",
      "\n",
      "avg / total       0.74      0.74      0.74     20000\n",
      "\n",
      "SVM kernel='poly' - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.71      0.69      0.70     10000\n",
      "   Negative       0.70      0.71      0.70     10000\n",
      "\n",
      "avg / total       0.70      0.70      0.70     20000\n",
      "\n",
      "SVM kernel='poly' - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.71      0.69      0.70     10000\n",
      "   Negative       0.70      0.71      0.70     10000\n",
      "\n",
      "avg / total       0.70      0.70      0.70     20000\n",
      "\n",
      "SVM kernel='linear' - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.74      0.74      0.74     10000\n",
      "   Negative       0.74      0.74      0.74     10000\n",
      "\n",
      "avg / total       0.74      0.74      0.74     20000\n",
      "\n",
      "SVM kernel='linear' - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.74      0.74      0.74     10000\n",
      "   Negative       0.74      0.74      0.74     10000\n",
      "\n",
      "avg / total       0.74      0.74      0.74     20000\n",
      "\n",
      "SVM kernel='rbf' - Bag of Words: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.98      0.02      0.04     10000\n",
      "   Negative       0.51      1.00      0.67     10000\n",
      "\n",
      "avg / total       0.74      0.51      0.36     20000\n",
      "\n",
      "SVM kernel='rbf' - TFIDF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.00      0.00      0.00     10000\n",
      "   Negative       0.50      1.00      0.67     10000\n",
      "\n",
      "avg / total       0.25      0.50      0.33     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "lr15_bow_report=classification_report(s15sentiment_test_data,s15lr_bow_predict,target_names=['Positive','Negative'])\n",
    "lr15_tfidf_report=classification_report(s15sentiment_test_data,s15lr_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(\"Logistic Regression - Bag of Words: \\n\",lr15_bow_report)\n",
    "print(\"Logistic Regression - TFIDF: \\n\",lr15_tfidf_report)\n",
    "\n",
    "#Multinomial Naives Bayes\n",
    "mnb15_bow_report=classification_report(s15sentiment_test_data,s15MNB_bow_predict,target_names=['Positive','Negative'])\n",
    "mnb15_tfidf_report=classification_report(s15sentiment_test_data,s15MNB_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(\"Multinomial Naives Bayes - Bag of Words: \\n\",mnb15_bow_report)\n",
    "print(\"Multinomial Naives Bayes - TFIDF: \\n\",mnb15_tfidf_report)\n",
    "\n",
    "#Support Vector Machines - Poly\n",
    "svc15_poly_bow_report=classification_report(s15sentiment_test_data,svc15_bow_pred,target_names=['Positive','Negative'])\n",
    "svc15_poly_tfidf_report=classification_report(s15sentiment_test_data,svc15_tfidf_pred,target_names=['Positive','Negative'])\n",
    "print(\"SVM kernel='poly' - Bag of Words: \\n\",svc15_poly_bow_report)\n",
    "print(\"SVM kernel='poly' - TFIDF: \\n\",svc15_poly_tfidf_report)\n",
    "\n",
    "#Support Vector Machines - Linear\n",
    "svc15_linear_bow_report=classification_report(s15sentiment_test_data,svc15_bow_pred1,target_names=['Positive','Negative'])\n",
    "svc15_linear_tfidf_report=classification_report(s15sentiment_test_data,svc15_tfidf_pred1,target_names=['Positive','Negative'])\n",
    "print(\"SVM kernel='linear' - Bag of Words: \\n\",svc15_linear_bow_report)\n",
    "print(\"SVM kernel='linear' - TFIDF: \\n\",svc15_linear_tfidf_report)\n",
    "\n",
    "#Support Vector Machines - rbf\n",
    "svc15_rbf_bow_report=classification_report(s15sentiment_test_data,svc15_rbf_bow_pred,target_names=['Positive','Negative'])\n",
    "svc15_rbf_tfidf_report=classification_report(s15sentiment_test_data,svc15_rbf_tfidf_pred,target_names=['Positive','Negative'])\n",
    "print(\"SVM kernel='rbf' - Bag of Words: \\n\",svc15_rbf_bow_report)\n",
    "print(\"SVM kernel='rbf' - TFIDF: \\n\",svc15_rbf_tfidf_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "\n",
    "- When the size of the training data is increased, a significant increase in accuracy is observed in the following algorithms:\n",
    "    - Bag Of Words Vectoriser\n",
    "        - Logistic Regression - 74.28%\n",
    "        - Multinomial Naives Bayes - 74.30%\n",
    "        - Support Vector Machines: Linear Kernel - 74.35%\n",
    "        - Support Vector Machines: Polynomial Kernel - 70.70%\n",
    "    - TF-IDF Vectoriser\n",
    "        - Logistic Regression - 74.25%\n",
    "        - Multinomial Naives Bayes - 74.30%\n",
    "        - Support Vector Machines: Linear Kernel - 74.35%\n",
    "        - Support Vector Machines: Polynomial Kernel - 70.70%\n",
    "\n",
    "\n",
    "\n",
    "- For all the above mentioned algorithms, by looking at the classification report (precision, recall and f1 score), we can therefore conclude that:\n",
    "    - For the Bag of Words vectorised model, Support Vector Machines (linear kernel) shows a significant increase in accuracy (to 74.35%) when the training data is increased with a precision score of 74%\n",
    "    - For the TF-IDF vectorised model, Support Vector Machines (linear kernel) shows a significant increase in accuracy (to 74.35%) when the training data is increased with a precision score of 74%\n",
    "\n",
    "\n",
    "\n",
    "- On the other hand, since the computing time is relatively higher for SVM compared to Logistic Regression and Naives Bayes algorithms, with a constraint in supporting hardware, it would be advisable to go for either Multinomial Naives Bayes or Logistic Regression models.\n",
    "\n",
    "\n",
    "- Also, with a powerful GPU, more advanced algorithms such as ensemble learning can be applied in order to improve the accuracy without taking much of computing time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
